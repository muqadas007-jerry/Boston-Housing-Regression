import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt

# Load dataset
df = pd.read_csv("https://raw.githubusercontent.com/selva86/datasets/master/BostonHousing.csv")

# Handling missing values
df.fillna(df.median(), inplace=True)

# Normalize numerical features
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
df[df.columns] = scaler.fit_transform(df[df.columns])

# Splitting data
X = df.drop(columns=["medv"])
y = df["medv"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Linear Regression from scratch
class LinearRegressionScratch:
    def __init__(self, learning_rate=0.01, epochs=1000):
        self.learning_rate = learning_rate
        self.epochs = epochs
        self.weights = None
        self.bias = None

    def fit(self, X, y):
        n_samples, n_features = X.shape
        self.weights = np.zeros(n_features)
        self.bias = 0

        for _ in range(self.epochs):
            y_pred = np.dot(X, self.weights) + self.bias
            dw = (1 / n_samples) * np.dot(X.T, (y_pred - y))
            db = (1 / n_samples) * np.sum(y_pred - y)
            self.weights -= self.learning_rate * dw
            self.bias -= self.learning_rate * db

    def predict(self, X):
        return np.dot(X, self.weights) + self.bias

# Train Linear Regression model
lr_model = LinearRegressionScratch(learning_rate=0.01, epochs=1000)
lr_model.fit(X_train.values, y_train.values)
y_pred_train = lr_model.predict(X_train.values)
y_pred_test = lr_model.predict(X_test.values)

# Compute RMSE and R^2
rmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))
r2_train = r2_score(y_train, y_pred_train)
rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))
r2_test = r2_score(y_test, y_pred_test)

print(f"Linear Regression - Train RMSE: {rmse_train}, R2: {r2_train}")
print(f"Linear Regression - Test RMSE: {rmse_test}, R2: {r2_test}")

# Random Forest from scratch
from collections import Counter

class DecisionTree:
    def __init__(self, max_depth=10, min_samples_split=2):
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.tree = None

    def fit(self, X, y):
        self.tree = self._grow_tree(X, y, depth=0)

    def _grow_tree(self, X, y, depth):
        if depth >= self.max_depth or len(y) < self.min_samples_split:
            return Counter(y).most_common(1)[0][0]
        
        best_feat = np.random.randint(X.shape[1])
        threshold = np.median(X[:, best_feat])
        left_idx = X[:, best_feat] < threshold
        right_idx = ~left_idx
        
        return {
            'feature': best_feat,
            'threshold': threshold,
            'left': self._grow_tree(X[left_idx], y[left_idx], depth + 1),
            'right': self._grow_tree(X[right_idx], y[right_idx], depth + 1)
        }

    def predict_one(self, x, tree):
        if not isinstance(tree, dict):
            return tree
        if x[tree['feature']] < tree['threshold']:
            return self.predict_one(x, tree['left'])
        return self.predict_one(x, tree['right'])

    def predict(self, X):
        return np.array([self.predict_one(x, self.tree) for x in X])

# Train Random Forest model
class RandomForestScratch:
    def __init__(self, n_trees=10, max_depth=10, min_samples_split=2):
        self.n_trees = n_trees
        self.trees = [DecisionTree(max_depth, min_samples_split) for _ in range(n_trees)]

    def fit(self, X, y):
        for tree in self.trees:
            sample_idx = np.random.choice(len(X), len(X), replace=True)
            X_sample, y_sample = X[sample_idx], y[sample_idx]
            tree.fit(X_sample, y_sample)

    def predict(self, X):
        tree_preds = np.array([tree.predict(X) for tree in self.trees])
        return np.mean(tree_preds, axis=0)

rf_model = RandomForestScratch(n_trees=10, max_depth=10)
rf_model.fit(X_train.values, y_train.values)
y_pred_rf = rf_model.predict(X_test.values)
rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))
r2_rf = r2_score(y_test, y_pred_rf)
print(f"Random Forest - Test RMSE: {rmse_rf}, R2: {r2_rf}")

# Feature Importance Visualization
feature_importance = np.random.rand(X_train.shape[1])
plt.figure(figsize=(10, 5))
plt.bar(X.columns, feature_importance)
plt.xticks(rotation=90)
plt.xlabel("Features")
plt.ylabel("Importance")
plt.title("Feature Importance (Random Values for Demonstration)")
plt.show()
